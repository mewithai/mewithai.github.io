---
last_modified_at: 2024-03-24 13:00:00 
title: "특정 언론사 News Agent Tools로 가지고 오기"
excerpt: "News를 가지고 와서 List로 정리해보자."
toc: true
toc_sticky: true
toc_label: "특정 언론사 News"
categories:
  - langchain
tags:
  - 자동화Bot
  - langchain
permalink: /langchain/Importing-specific-media-outlet/
---

## 배경과 목표

네이버 News API를 사용해서 News 본문을 얻으면 기사와 다른 내용들(광고, 다른 기사 링크)도 포함된다.
이런 것들을 제외시킬려면 각 언론사의 홈페이지에 TAG를 각각 분석해서 Reader를 만들어야 한다.
그래서 그냥 몇개의 언론사를 정하고 거기에서만 기사를 읽기로 하였다.

### 목표

- 특정 언론사의 News 리스트를 얻을 수 있는 Agent Tool을 만들 것

## 코드

네이버 News API를 통해서 얻었던 `Naver News Search Class`를 `매일경제`로 바꿔보았다.

```python
class NaverNews():
    def __init__(self):
        self.baseurl = "https://s.search.naver.com/p/newssearch/search.naver"
        self.urlparameter = {
            'where': 'news_tab_api',
            'start': '1',
            'query': '###',
            'field': '0',
            'is_sug_officeid': '0',
            'mynews': '1',
            'news_office_checked': '1009', #매일경제
            'office_category': '0', 
            'office_section_code': '3',
            'office_type': '1', 
            'pd': '4',
            'photo': '0',
            'service_area': '1',
            'sort': '1',
            'spq': '0'
        }

    def search_news(self, query):
        self.nextUrl = "First Loop"
        self.urlparameter['start'] =  -9
        self.urlparameter['query'] =  query
        
        # initialize result
        result = []

        while self.nextUrl != "":
            self.urlparameter['start'] += 10
            logger.info(f"Start Parameter: {self.urlparameter['start']}")
            # 딕셔너리를 URL 쿼리 문자열로 변환
            query_string = urlencode(self.urlparameter)

            # 최종 URL 생성
            SearchUrl = f"{self.baseurl}?{query_string}"
            logger.info(f"SearchUrl: {SearchUrl}")

            # requests를 사용하여 URL로부터 데이터 가져오기
            response = requests.get(SearchUrl)

            # 성공적으로 데이터를 가져왔는지 확인 (HTTP 상태 코드가 200인지)
            if response.status_code == 200:
                
                # BeautifulSoup 객체 생성, HTML 파싱
                siteJson = json.loads(response.content)

                # Set if next loop is needed
                self.nextUrl = siteJson['nextUrl']

                # Error handling contents is empty
                if len(siteJson['contents']) == 0:
                    return result

                for li in siteJson['contents']:
                    # siteJson에는 contents와 nextUrl이 있다.
                    soup = BeautifulSoup(li, 'html.parser')

                    # `URL링크`는 class="news_tit"의 `href`을 뽑아내면 된다.
                    # `Title`은 class="news_tit"의 `title` TAG를 뽑는다.
                    #  `Description`은 class="api_txt_lines dsc_txt_wrap" 내용을 뽑는다.

                    link = soup.find(class_='news_tit')['href']
                    title = soup.find(class_='news_tit')['title']
                    description = soup.find(class_='api_txt_lines').text
                    
                    # Add into result {link, title, description}
                    result.append({'link': link, 'title': title, 'description': description})

        return result
```

 네이버 홈에서 가지고 와야하는데 중간에 AJAX가 있는 것을 발견 그 URL을 그대로 reqeust해 왔다.

 User-Agent 때문에 차단될 수 있을 듯 한데, 그때는 Chrome Headless Proxy를 쓸 것이다.

- 미래의 나에게: 관련문서는 SecondBrain에 정리했으니까, 가서 찾아봐라.

### 실행

- News Search Agent에 새 tool을 주고 동작을 시켰다.

```
> Entering new CrewAgentExecutor chain...
I should use the get News from naver tool to search for "병원" and get a list of news related to it.

Action: get News from naver
Action Input: {"keyword": "병원", "num_news": 10}

News about 병원 are [ 따라라라라라라라라 ].

Final Answer: A list of news related to 병원, including title, link, and description.

> Finished chain.
```

- 새로운 Tool을 Agent가 사용하였다.
- 이상한 점
  - 지난 Tools에서는 가져와야할 News 갯수를 지정해줬는데, 이번 Tool에서는 그걸 제외시켰다.
  - 하지만, `Action Input`으로 지난번 Parameter를 넣고 있다.
  - 혹시, 이거 `API_KEY`를 기반으로 이용자의 행동패턴을 학습하는거 아닌가? 하는 의문이 든다.

### 이슈사항

이렇게 해서 돌려보면 Parameter 상에서는 항상 최근 1일 사이에 입력된 기사가 넘어와야하는데, 막상 해보면 몇일 전 기사도 넘어온다.

그건, 네이버에 기사를 Refresh할 목적인지 `기사를 수정하면 그것이 새롭게 입력된 기사와 같이 취급`되는 듯 하다.

![]({{ site.url }}{{ site.baseurl }}/assets/images/20240324132734.png)

이것은 중복된 기사가 수집될 수 있기 때문에 중복제거 프로세스가 필요하다.

## 결론 및 Next Step

- Naver에서 특정 언론사의 기사 리스트를 가지고 오도록 하였다.
- 이를 LangChain Tool로 구현하고 Agent를 구동, 동작함을 확인하였다.
- 기사를 수정하는 것도 신규 기사 입력과 동일 하게 취급되어 추가 작업이 필요하다.

### Next Step

- 기사의 내용이 내가 원하는 내용과 일치하는 지 확인하고 맞다면 요약하는 기능 시험
- Writer Agent를 활성화 시키고 기본 기능을 만들어 볼 것